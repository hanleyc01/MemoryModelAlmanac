"""Implementation of bidirectional associative memories."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_bidirectional.ipynb.

# %% auto 0
__all__ = ['NDRAM', 'fit_ndram']

# %% ../nbs/02_bidirectional.ipynb 4
import equinox as eqx
import jax
import jax.lax as lax
import jax.numpy as jnp
import jax.random as jr
import numpy as np
from beartype import beartype as typechecker
from beartype import typing as T
from fastcore.basics import *
from fastcore.meta import *
from jaxtyping import Array, Float, jaxtyped
from einops import rearrange

# %% ../nbs/02_bidirectional.ipynb 6
@jaxtyped(typechecker=typechecker)
class NDRAM(eqx.Module):
    """Nonlinear Dynamic Recurrent Associative Memory for Learning Bipolar and Nonpolar Correlated Patterns."""

    W: Float[Array, "D D"]  # The weights of the network
    lr: float  # Learning rate parameter
    tr: float  # Transmission parameter

    @classmethod
    @jaxtyped(typechecker=typechecker)
    def init(
        cls,
        pattern_dim: int,  # The dimension of the patterns the network will be learning and recalling.
        lr: float,  # The learning rate for the weight update step.
        transmission: float,  # Transmission parameter.
    ):
        """Initialize an empty NDRAM network."""
        return cls(
            W=jnp.zeros((pattern_dim, pattern_dim)), lr=lr, transmission=transmission
        )

    @staticmethod
    @jaxtyped(typechecker=typechecker)
    def weight_update(
        weights: Float[Array, "D D"],  # The weights of the network.
        query0: Float[Array, " D"],  # The initial query state time t = 0
        queryt: Float[Array, " D"],  # The current query state at time t
        lr: float,  # The learning rate parameter
    ):
        """Perform a weight update."""
        query0 = rearrange(query0, "d -> d 1")
        queryt = rearrange(queryt, "d -> d 1")
        return weights + (lr * ((query0 @ query0.T) - (queryt @ queryt.T)))

    @staticmethod
    @jaxtyped(typechecker=typechecker)
    def query_update(
        weights: Float[Array, "D D"],  # The weights of the network
        query: Float[Array, " D"],  # The query state
        transmission: float,  # The transmission parameter
        num_transmissions=1,  # Number of transmissions to perform.
    ):
        """Perform a query update."""

        def update_step(query, _i):
            acts = weights @ query

            def f(a_i):
                """Transmission function on activations."""
                gt_1 = a_i > 1.0
                lt_n1 = a_i < -1.0
                return lax.cond(
                    gt_1,
                    lambda: 1.0,
                    lambda: lax.cond(
                        lt_n1,
                        lambda: -1.0,
                        lambda: ((transmission + 1) * a_i) - (transmission * (a_i**3)),
                    ),
                )

            queryt = jax.vmap(f)(acts)
            return queryt, queryt

        iters = np.arange(num_transmissions)
        final_act, _ = lax.scan(update_step, query, iters)
        return final_act

    @eqx.filter_jit
    @jaxtyped(typechecker=typechecker)
    def __call__(self, query: Float[Array, " D"]) -> Float[Array, " D"]:
        """Perform one step recall."""
        return NDRAM.query_update(self.W, query, self.tr)

# %% ../nbs/02_bidirectional.ipynb 7
@jaxtyped(typechecker=typechecker)
def fit_ndram(
    Xi: Float[Array, "N D"],  # The patterns to learn
    lr: float,  # The learning rate parameter
    tr: float,  # The transmission rate parameter
    num_transmissions: int = 1,  # Optional, number of transmission steps to undertake. Default: `1`.
    nsteps: int = 2_000,  # Number of training steps to undertake. Default: `2_000`.
    key: jax.Array = jr.PRNGKey(0),  # Optional, default `jr.PRNGKey(0)`.
):
    """Fit an NDRAM model to the dataset, returning a trained model."""
    are_iters = nsteps is None
    D = Xi.shape[-1]
    init_W = jnp.zeros(shape=(D, D), dtype=jnp.float32)

    def scan_update_step(W, query):
        query_t = NDRAM.query_update(
            W, query, transmission=tr, num_transmissions=num_transmissions
        )
        W_t = NDRAM.weight_update(W, query0=query, queryt=query_t, lr=lr)
        return (W_t, W_t)

    prototypes = jr.choice(key, Xi, shape=(nsteps,))
    final_W, Ws = lax.scan(scan_update_step, init=init_W, xs=prototypes)
    return NDRAM(final_W, lr=lr, tr=tr), None

