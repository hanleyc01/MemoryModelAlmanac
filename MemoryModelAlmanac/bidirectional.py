"""Implementation of bidirectional associative memories."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03_bidirectional.ipynb.

# %% auto 0
__all__ = ['NDRAM', 'fit_ndram', 'Chartier', 'fit_chartier']

# %% ../nbs/03_bidirectional.ipynb 4
import equinox as eqx
import jax
import jax.lax as lax
import jax.numpy as jnp
import jax.random as jr
import numpy as np
from beartype import beartype as typechecker
from beartype import typing as T
from fastcore.basics import *
from fastcore.meta import *
from jaxtyping import Array, Float, jaxtyped
from einops import rearrange

# %% ../nbs/03_bidirectional.ipynb 6
@jaxtyped(typechecker=typechecker)
class NDRAM(eqx.Module):
    """Nonlinear Dynamic Recurrent Associative Memory for Learning Bipolar and Nonpolar Correlated Patterns."""

    W: Float[Array, "D D"]  # The weights of the network
    lr: float  # Learning rate parameter
    tr: float  # Transmission parameter

    @staticmethod
    @jaxtyped(typechecker=typechecker)
    def weight_update(
        weights: Float[Array, "D D"],  # The weights of the network.
        query_0: Float[Array, " D"],  # The initial query state time t = 0
        query_t: Float[Array, " D"],  # The current query state at time t
        lr: float,  # The learning rate parameter
    ):
        """Perform a weight update."""
        query_0 = rearrange(query_0, "d -> d 1")
        query_t = rearrange(query_t, "d -> d 1")
        return weights + (lr * ((query_0 @ query_0.T) - (query_t @ query_t.T)))

    @staticmethod
    @jaxtyped(typechecker=typechecker)
    def query_update(
        weights: Float[Array, "D D"],  # The weights of the network
        query: Float[Array, " D"],  # The query state
        tr: float,  # The transmission parameter
        num_transmissions=1,  # Number of transmissions to perform.
    ):
        """Perform a query update."""

        def update_step(query, _i):
            acts = weights @ query

            def f(a_i):
                """Transmission function on activations."""
                gt_1 = a_i > 1.0
                lt_n1 = a_i < -1.0
                return lax.cond(
                    gt_1,
                    lambda: 1.0,
                    lambda: lax.cond(
                        lt_n1,
                        lambda: -1.0,
                        lambda: ((tr + 1) * a_i) - (tr * (a_i**3)),
                    ),
                )

            query_t = jax.vmap(f)(acts)
            return query_t, None

        iters = np.arange(num_transmissions)
        final_act, _ = lax.scan(update_step, query, iters)
        return final_act

    @eqx.filter_jit
    @jaxtyped(typechecker=typechecker)
    def __call__(self, query: Float[Array, " D"]) -> Float[Array, " D"]:
        """Perform one step recall."""
        return NDRAM.query_update(self.W, query, self.tr)

# %% ../nbs/03_bidirectional.ipynb 7
@jaxtyped(typechecker=typechecker)
def fit_ndram(
    Xi: Float[Array, "N D"],  # The patterns to learn
    lr: float,  # The learning rate parameter
    tr: float,  # The transmission rate parameter
    num_transmissions: int = 1,  # Optional, number of transmission steps to undertake. Default: `1`.
    nsteps: int = 2_000,  # Number of training steps to undertake. Default: `2_000`.
    key: jax.Array = jr.PRNGKey(0),  # Optional, default `jr.PRNGKey(0)`.
):
    """Fit an NDRAM model to the dataset, returning a trained model."""
    D = Xi.shape[-1]
    init_W = jnp.zeros(shape=(D, D), dtype=jnp.float32)

    def scan_update_step(W, query):
        query_t = NDRAM.query_update(
            W, query, tr=tr, num_transmissions=num_transmissions
        )
        W_t = NDRAM.weight_update(W, query_0=query, query_t=query_t, lr=lr)
        return (W_t, W_t)

    prototypes = jr.choice(key, Xi, shape=(nsteps,))
    final_W, Ws = lax.scan(scan_update_step, init=init_W, xs=prototypes)
    return NDRAM(final_W, lr=lr, tr=tr), None


# %% ../nbs/03_bidirectional.ipynb 17
@jaxtyped(typechecker=typechecker)
class Chartier(eqx.Module):
    """Bidirectional Hetero-associative Memory."""

    W: Float[Array, "D D"]  # "Forward" weights
    V: Float[Array, "D D"]  # "Backward" weights
    lr: float  # Learning rate parameter
    tr: float  # Transmission parameter

    @staticmethod
    @jaxtyped(typechecker=typechecker)
    def forward_weight_update(
        W: Float[Array, "D D"],  # The "forward" weights
        x_0: Float[Array, " D"],  # Initial query, i.e. query at time `t = 0`
        x_t: Float[Array, " D"],  # Query at time `t`
        y_0: Float[Array, " D"],  # Initial target, target at time `t = 0`.
        y_t: Float[Array, " D"],  # Target at time `t`.
        lr: float,  # Learning rate parameter
    ) -> Float[Array, "D D"]:  # Updated "forward" weights
        """Forward weight update."""
        x_0 = rearrange(x_0, "d -> d 1")
        x_t = rearrange(x_t, "d -> d 1")
        y_0 = rearrange(y_0, "d -> d 1")
        y_t = rearrange(y_t, "d -> d 1")
        W_t = W + lr * (y_0 - y_t) @ (x_0 - x_t).T
        return W_t

    @staticmethod
    @jaxtyped(typechecker=typechecker)
    def backward_weight_update(
        V: Float[Array, "D D"],  # The "backward" weights
        x_0: Float[Array, " D"],  # Initial query, i.e. query at time `t = 0`
        x_t: Float[Array, " D"],  # Query at time `t`
        y_0: Float[Array, " D"],  # Initial target, target at time `t = 0`.
        y_t: Float[Array, " D"],  # Target at time `t`.
        lr: float,  # Learning rate parameter
    ) -> Float[Array, "D D"]:  # Updated backward weights
        x_0 = rearrange(x_0, "d -> d 1")
        x_t = rearrange(x_t, "d -> d 1")
        y_0 = rearrange(y_0, "d -> d 1")
        y_t = rearrange(y_t, "d -> d 1")
        V_t = V + lr * (x_0 - x_t) @ (y_0 - y_t).T
        return V_t

    @staticmethod
    @jaxtyped(typechecker=typechecker)
    def _forward_prediction(
        W: Float[Array, "D D"],  # The "forward" weights
        x_0: Float[Array, " D"],  # The forward query
        tr: float,  # The transmission rate
        num_transmissions: int = 1,  # The number of transmissions to perform
    ) -> Float[Array, " D"]:  # The recalled value, after `num_transmissions`
        """Retrieve an estimated target from a query."""

        def update_step(query, _i):
            acts = W @ query

            def f(a_i):
                """Transmission function on activations."""
                gt_1 = a_i > 1.0
                lt_n1 = a_i < -1.0
                return lax.cond(
                    gt_1,
                    lambda: 1.0,
                    lambda: lax.cond(
                        lt_n1,
                        lambda: -1.0,
                        lambda: ((tr + 1) * a_i) - (tr * (a_i**3)),
                    ),
                )

            query_t = jax.vmap(f)(acts)
            return query_t, None

        iters = np.arange(num_transmissions)
        final_act, _ = lax.scan(update_step, x_0, iters)
        return final_act

    @staticmethod
    @jaxtyped(typechecker=typechecker)
    def _backward_prediction(
        W: Float[Array, "D D"],  # The "backward" weights
        y_0: Float[Array, " D"],  # The backward query
        tr: float,  # The transmission rate
        num_transmissions: int = 1,  # The number of transmissions to perform
    ) -> Float[Array, " D"]:  # The recalled value, after `num_transmissions`
        """Retrieve an estimated target from a query."""

        def update_step(query, _i):
            acts = W @ query

            def f(a_i):
                """Transmission function on activations."""
                gt_1 = a_i > 1.0
                lt_n1 = a_i < -1.0
                return lax.cond(
                    gt_1,
                    lambda: 1.0,
                    lambda: lax.cond(
                        lt_n1,
                        lambda: -1.0,
                        lambda: ((tr + 1) * a_i) - (tr * (a_i**3)),
                    ),
                )

            query_t = jax.vmap(f)(acts)
            return query_t, None

        iters = np.arange(num_transmissions)
        final_act, _ = lax.scan(update_step, y_0, iters)
        return final_act

    @jaxtyped(typechecker=typechecker)
    def forward_pred(
        self,
        x: Float[Array, " D"],  # The forward query.
        num_transmissions: int = 1,  # The number of transmissions to perform.
    ) -> Float[Array, " D"]:  # The recalled target after `num_transmissions`
        """Perform "forwards" recall."""
        return Chartier._forward_prediction(
            self.W, x, tr=self.tr, num_transmissions=num_transmissions
        )

    @jaxtyped(typechecker=typechecker)
    def backward_pred(
        self,
        y: Float[Array, " D"],  # The backwards query
        num_transmissions: int = 1,  # The number of transmissions to perform
    ) -> Float[Array, " D"]:  # The recalled query after `num_transmissions`.
        """Perform "backwards" recalll."""
        return Chartier._backward_prediction(
            self.V, y, tr=self.tr, num_transmissions=num_transmissions
        )

# %% ../nbs/03_bidirectional.ipynb 18
@jaxtyped(typechecker=typechecker)
def fit_chartier(
    A: Float[Array, "N D"],  # The "address" matrix of patterns to store
    P: Float[Array, "N D"],  # The "backwards" matrix of patterns to store
    lr: float,  # The learning rate
    tr: float,  # The transmission rate
    nsteps: int = 2_000,  # The number of learning steps to perform
    num_transmissions: int = 1,  # The number of transmission steps to perform
    key: jax.Array = jr.PRNGKey(0),  # Optional, default `jr.PRNGKey(0)`
):
    """Create and fit a `Chartier` network to the address and pattern matrices `A` and `P`."""
    N, D = A.shape
    M = P.shape[-1]
    init_W = jnp.zeros(shape=(D, D), dtype=jnp.float32)
    init_V = jnp.zeros(shape=(M, M), dtype=jnp.float32)

    def scan_update_step(weights, i):
        W, V = weights
        x_0 = A[i, :]
        y_0 = P[i, :]
        y_t = Chartier._forward_prediction(
            W, x_0, tr=tr, num_transmissions=num_transmissions
        )
        x_t = Chartier._backward_prediction(
            V, y_0, tr=tr, num_transmissions=num_transmissions
        )
        W_t = Chartier.forward_weight_update(W, x_0, x_t, y_0, y_t, lr)
        V_t = Chartier.backward_weight_update(V, x_0, x_t, y_0, y_t, lr)
        return (W_t, V_t), None

    idxs = jr.choice(key, jnp.arange(N), shape=(nsteps,))
    (final_W, final_V), _ = lax.scan(scan_update_step, init=(init_W, init_V), xs=idxs)
    return Chartier(final_W, final_V, tr, lr)
